# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J7vWnflR3HF6mNrcJxJ27EJU0Khezyki

# **Mounting Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Import Library**"""

import keras
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from keras import layers
from keras import ops

"""# **Downloading Datasets**"""

from google.colab import files
files.upload()

!kaggle datasets download -d arashnic/book-recommendation-dataset -p /content/drive/MyDrive/ML-Project/Datasets
!unzip /content/drive/MyDrive/ML-Project/Datasets/book-recommendation-dataset.zip -d /content/drive/MyDrive/ML-Project/Datasets

"""# **Loading Datasets**"""

books = pd.read_csv('/content/drive/MyDrive/ML-Project/Datasets/Books.csv')
ratings = pd.read_csv('/content/drive/MyDrive/ML-Project/Datasets/Ratings.csv')
users = pd.read_csv('/content/drive/MyDrive/ML-Project/Datasets/Users.csv')

jml_baris, jml_kolom = books.shape
print('jumlah baris', jml_baris)
print('jumlah kolom', jml_kolom)

books.head()

jml_baris, jml_kolom = ratings.shape
print('jumlah baris', jml_baris)
print('jumlah kolom', jml_kolom)

ratings.head()

jml_baris, jml_kolom = users.shape # Mendapatkan jumlah baris dan kolom
print('jumlah baris', jml_baris) # Menampilkan jumlah baris
print('jumlah kolom', jml_kolom) # Menampilkan jumlah kolom

users.head() # Menampilkan 5 baris teratas

"""# *Exploratory Data Analysis(EDA)*

"""

books.describe()
ratings.describe()
users.describe()

"""## Books"""

books.info()

"""Mengubah tipe data Year-Of-Publication yang sebelumnya bertype object menjadi integer"""

# jika langsung menjalankan kode ini maka terjadi error karena ada kesalahan input
# books['Year-Of-Publication'].astype('int')

# Year-Of-Publication yang terjadi kesalahan input
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

# menghapus data yang terjadi kesalahan input
temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)

# mengubah tipe data menjadi integer
books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

"""menghapus kolom yang tidak diperlukan"""

books = books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1)
books.head()

# melihat berapa banyak entri dari masing - masing variabel
print("Jumlah nomor ISBN Buku:", len(books['ISBN'].unique()))
print("Jumlah judul buku:", len(books['Book-Title'].unique()))
print('Jumlah penulis buku:', len(books['Book-Author'].unique()))
print('Jumlah Tahun Publikasi:', len(books['Year-Of-Publication'].unique()))
print('Jumlah nama penerbit:', len(books['Publisher'].unique()))

# Grouping'Book-Author' dan hitung jumlah buku yang ditulis oleh masing-masing penulis
author_counts = books.groupby('Book-Author')['Book-Title'].count()

# Urutkan penulis dalam urutan menurun
sorted_authors = author_counts.sort_values(ascending=False)

# Pilih 10 penulis teratas
top_10_authors = sorted_authors.head(10)

# Plot 10 penulis teratas dan buku yang ditulis oleh penulis kemudian dihitung menggunakan plot batang
plt.figure(figsize=(12, 6))
top_10_authors.plot(kind='bar')
plt.xlabel('Nama Penulis')
plt.ylabel('Jumlah Buku')
plt.title('10 Penulis Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Ratings"""

ratings.info()

"""Melihat jumlah entri dari masing - masing variabel"""

print('Jumlah User-ID:', len(ratings['User-ID'].unique()))
print('Jumlah buku berdasarkan ISBN:', len(ratings['ISBN'].unique()))

print('Jumlah rating buku:')
sorted_ratings = ratings['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_ratings.index, 'Jumlah': sorted_ratings.values})

"""Jika dilihat pada informasi sebelumnya, dataset ratings memiliki 1149780 baris data. Guna menghemat memory maka hanya akan mengambil sampel sebanyak 30000 data saja"""

df_ratings = ratings[:30000]
df_ratings

"""## Users"""

users.info()

"""Dari informasi diatas, dataset user berisi User-ID, lokasi(Location) dan umur(Age). Data tersebut digunakan jika ingin membuat sistem rekomendasi berdasarkan demografi atau kondisi sosial pengguna. Pada sistem rekomendasi kali ini tidak menerapkannya yang digunakan hanya books dan ratings saja

# Data Preprocessing

Menggabungkan dataset books dan rating menjadi satu kesatuan
"""

books = pd.merge(ratings, books, on='ISBN', how='left')
books

# menampilkan rating berdasarkan ISBN
books.groupby('ISBN').sum()

"""Menghapus missing value"""

books.isnull().sum()

# menghapus data yang memiliki NaN
clean_books = books.dropna()

clean_books.isnull().sum()

len(clean_books['ISBN'].unique())

len(clean_books['Book-Title'].unique())

"""Dari hasil diatas dapat disimpulkan ISBN yang berbeda memiliki Book-Title yang sama atau terjadi duplikasi. Maka perlu melakukan proses menghapus data yang duplikat dengan kode dibawah"""

clean_books = clean_books.drop_duplicates('Book-Title')
clean_books

"""# Data Preparation untuk model Colaborative Filtering

Encoding
"""

# fungsi untuk encoding data
def encoding(data_series):
    data = data_series.unique().tolist()
    encoded = {x: i for i, x in enumerate(data)}
    return encoded
# fungsi untuk decoding data
def decoding(data_series):
    data = data_series.unique().tolist()
    decoded = {i: x for i, x in enumerate(data)}
    return decoded

user_encoding = encoding(df_ratings['User-ID'])
isbn_encoding = encoding(df_ratings['ISBN'])

df_ratings['user'] = df_ratings['User-ID'].map(user_encoding)
df_ratings['book_title'] = df_ratings['ISBN'].map(isbn_encoding)

df_ratings.head()

# num_user
num_user = len(user_encoding)
print(f"Number of User : {num_user}")
# num_book_title
num_book = len(isbn_encoding)
print(f"Number of Book : {num_book}")

# mengubah nilai rating menjadi float
df_ratings['Book-Rating'] = df_ratings['Book-Rating'].values.astype(np.float32)

# nilai minimum rating
min_rating = min(df_ratings['Book-Rating'])
# nilai maksimum rating
max_rating = max(df_ratings['Book-Rating'])

print(df_ratings.shape[0])

"""# Modeling

## Content Base Filtering
"""

books = clean_books[:20000]
books = books.rename(columns={'Book-Title': 'title', 'Book-Author': 'author'})

print('Jumlah data buku:', len(books.ISBN.unique()))
print('Jumlah data rating buku dari pembaca:', len(ratings.ISBN.unique()))
print('jumlah data pengguna:', len(users['User-ID'].unique()))

# Books variabel
# Menghapus value pada 'Year-Of-Publication' yang bernilai teks
books[(books['Year-Of-Publication'] == 'DK Publishing Inc')
      | (books['Year-Of-Publication'] == 'Gallimard')]

temp = (books['Year-Of-Publication'] ==
        'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc')
      | (books['Year-Of-Publication'] == 'Gallimard')]

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

books.head()

print("Jumlah nomor ISBN Buku:", len(books['ISBN'].unique()))
print("Jumlah judul buku:", len(books['title'].unique()))
print('Jumlah penulis buku:', len(books['author'].unique()))
print('Jumlah Tahun Publikasi:', len(books['Year-Of-Publication'].unique()))
print('Jumlah nama penerbit:', len(books['Publisher'].unique()))

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(books['author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(books['author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=books.title
).sample(15, axis=1).sample(10, axis=0)

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama judul buku
cosine_sim_df = pd.DataFrame(
    cosine_sim, index=books['title'], columns=books['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap judul buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Mendapatkan rekomendasi
def book_recommendation(book_title, similarity_data=cosine_sim_df, items=books[['title', 'author']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    if book_title not in similarity_data.columns:
        print(f"Book '{book_title}' not found in the similarity matrix.")
        return pd.DataFrame()

    index = similarity_data.loc[:, book_title].to_numpy(
    ).argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

# contoh judul buku
book_title_test = "She Said Yes: The Unlikely Martyrdom of Cassie Bernall"

books[books['title'].eq(book_title_test)]

# Mendapatkan rekomendasi judul buku yang mirip
book_recommendation(book_title_test)

# Evaluasi Model dengan Content Based Filtering
# Menentukan threshold untuk mengkategorikan similarity sebagai 1 atau 0
threshold = 0.5

# Membuat ground truth data dengan asumsi threshold
ground_truth = np.where(cosine_sim >= threshold, 1, 0)

# Menampilkan beberapa nilai pada ground truth matrix
ground_truth_df = pd.DataFrame(
    ground_truth, index=books['title'], columns=books['title']).sample(5, axis=1).sample(10, axis=0)


# Mengambil sebagian kecil dari cosine similarity matrix dan ground truth matrix
sample_size = 10000
cosine_sim_sample = cosine_sim[:sample_size, :sample_size]
ground_truth_sample = ground_truth[:sample_size, :sample_size]

# Mengonversi cosine similarity matrix menjadi array satu dimensi untuk perbandingan
cosine_sim_flat = cosine_sim_sample.flatten()

# Mengonversi ground truth matrix menjadi array satu dimensi
ground_truth_flat = ground_truth_sample.flatten()

# Menghitung metrik evaluasi
predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
    ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""## Collaborative Filtering

### Data Spliting
"""

x = df_ratings[['user', 'book_title']].values
y = df_ratings['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)

"""### Model Colaborative Filtering

Untuk model Colaborative Filtering saya mengambil referensi dari [keras](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) dengan menyesuaikan dengan kasus yang sedang dikerjakan.
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_book, embedding_size, dropout_rate=0.2, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users
        self.num_book = num_book
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding( # layer embedding user
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

        self.book_embedding = layers.Embedding( # layer embedding book_title
            num_book,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
        self.dropout = layers.Dropout(rate=dropout_rate)
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0]) # memanggil layer embedding 1
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2

        book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
        book_vector = self.dropout(book_vector)
        book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

        dot_user_book = ops.tensordot(user_vector, book_vector, 2) # perkalian dot product

        x = dot_user_book + user_bias + book_bias

        return ops.nn.sigmoid(x) # activation sigmoid

import tensorflow as tf

modelCF = RecommenderNet(num_user, num_book, 50)

modelCF.compile(optimizer=Adam(learning_rate=0.0001),
                loss='mse',
                metrics=[tf.keras.metrics.RootMeanSquaredError()])

historyCF = modelCF.fit(
    x_train,
    y_train,
    batch_size=16,
    epochs=50,
    validation_data=(x_test, y_test)
)

"""### Mendapatkan Rekomendasi"""

book = clean_books.copy()
book = book.rename(columns={'Book-Title': 'title', 'Book-Author': 'author'})

# mengambil sampel user
user_id = df_ratings['User-ID'].sample(1).iloc[0]
book_readed_by_user = df_ratings[df_ratings['User-ID'] == user_id]

# membuat variabel book_not_readed
book_not_readed = book[~book['ISBN'].isin(book_readed_by_user['ISBN'].values)]['ISBN']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_encoding.keys()))
)

book_not_readed = [[isbn_encoding.get(x)] for x in book_not_readed]
user_encoder = user_encoding.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_model = modelCF.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    book_not_readed[x][0] for x in top_ratings_indices
]

top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_rows = book[book['ISBN'].isin(top_book_user)]

# Menampilkan rekomendasi buku dalam bentuk DataFrame
book_rows_data = []
for row in book_rows.itertuples():
    book_rows_data.append([row.title, row.author])

recommended_isbn = [list(isbn_encoding.keys())[i] for i in recommended_book_ids]
recommended_book = book[book['ISBN'].isin(recommended_isbn)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.title, row.author])

# Membuat DataFrame untuk output
output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Menampilkan hasil rekomendasi dalam bentuk DataFrame
print("Showing recommendation for users: {}".format(user_id))
print("===" * 9)
print("Book with high ratings from user")
print("----" * 8)
print(df_book_readed_by_user)
print("----" * 8)
print("Top 10 books recommendation")
print("----" * 8)
df_recommended_books

plt.plot(historyCF.history['root_mean_squared_error'])
plt.plot(historyCF.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()